Splout SQL User Guide
=====================
:keywords: Splout, SQL, User, Guide, Hadoop, Database, Configuration, API, Server

Splout SQL installation
-----------------------

For running Splout SQL, the underlying system must have the following:

- Java 1.6 or higher.
- Hadoop (versions 0.20.X including CDH3 or stable 1.0.4). Splout has been tested to run successfully at least on 1.0.4, 0.20.205 and CDH3r5.

You must have HADOOP_HOME environment variable defined in your system. Splout will load the libraries from your Hadoop distribution in runtime.

When Splout SQL is uncompressed in a path we will refer to as SPLOUT_HOME, it can be used straight-away. However, the Configuration section has to be revised carefully in order to adjust everything properly.

Launching the server daemons is as easy as:

 bin/splout-service.sh qnode start
 bin/splout-service.sh dnode start

.PID files
TIP: The daemons generate a .pid file from where they are launched.

WARNING: By default, DNode data is written in “dnode-staging” in the same folder where it is launched. Please read carefully the Configuration section to override this default.

It is possible to have more than one QNode and more than one DNode in a single machine, but for that it is important to understand the Configuration and modify a few properties, specially if you launch the services from the same folder.

Logs are stored in logs/ folder from where daemons are launched. Apache commons-logging is used for logging.

.Splout SQL has been tested to perform correctly under both AMD64 and i386 default Amazon AMIs. Splout is compatible with the latest Elastic Map Reduce AMI.

Splout SQL is still beta, so if you find any strange problem or issue please contact us or raise a bug in Github.

The basics
----------

.Splout Terminology
* Table: A table in Splout can be seen as a standard database table. We will later see how tables are defined and what are their particularities.
* Tablespace: A tablespace in Splout is a logical union of one or more tables, which are co-partitioned in the same way. 
* Deploy: Splout "deploys" data from a Hadoop-compatible file system such as HDFS or S3, meaning that DNodes fetch the appropriate database binary files and save them in their local filesystem. When all DNodes have been coordinated to do so, the version of the database that they are serving changes atomically to the next one that has been fetched.
* Rollback: Splout can "rollback" previous versions if they are kept in the local storage of all DNodes. DNodes may keep up to some number of versions for each tablespace, which is a configurable property (see the Configuration section for that).

Table definitions
^^^^^^^^^^^^^^^^^

A table schema's is defined the same way a Pangool's tuple schema is defined. However, the data types are adjusted to match those which are compatible with SQLite. 
The following table shows the correspondence between a Pangool type and the underlying SQLite type used:

.Correspondence between Pangool types and SQLite types used
|=======================
| *Pangool type* | *SQLite type used* 
| INT | INTEGER
| LONG | INTEGER
| DOUBLE | REAL
| FLOAT | REAL
| STRING | TEXT
| BOOLEAN | INTEGER (0 is false, 1 is true. SQLite doesn't support booleans.)
|=======================
	
Table types and restrictions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A table is either partitioned or replicated to all partitions. However, a tablespace should have at least one partitioned table for the indexer to be able to distribute the data among partitions!

Partitioning
^^^^^^^^^^^^

The most usual case of partitioning is columnar partitioning, meaning that a table is partitioned using one or more columns of its schema.

When more than one table is partitioned in the same tablespace, they must be co-partitioned using the same kind of fields. For example, if a tablespace A contains tables A1, A2 and A3, and A1 and A2
are partitioned tables and A3 is replicated to all partitions, then if A1 is partitioned by a pair of (string, int) columns, then A2 should also be partitioned by a pair of (string, int) columns.

Note that when a table is partitioned by some columns, Splout just concatenates the value of those columns as a single string. From that point of view, partitioning is a function of a row that returns a string.
Therefore, it is also possible to partition using arbitrary functions, for example a javascript function that takes the first two characters of a field. You can check this in the Advanced API section.

Partitioning and Querying
^^^^^^^^^^^^^^^^^^^^^^^^^

Because data is explicitly partitioned, the user must also explicitly provide a partitioning key when querying Splout. 
For example, if a dataset has been partitioned by "customer_id", then the user will provide the appropriated "customer_id" together with the SQL query when querying Splout through its REST interface. 

Splout SQL Configuration
------------------------

Splout uses a dual-configuration method, one file for the defaults and one file for overriding them. All you need to do if you want to override a default is specify it in a “splout.properties” file. This file must be in SPLOUT_HOME or in any other place from where you will launch the daemons or in any location of the classpath.

This table shows the property names, the explanation of each property and its default value.

.Splout server configuration
|=======================
| qnode.port | The port this QNode will run on, | 4412
| qnode.port.autoincrement | Whether this QNode should find the next available port in case "dnode.port" is busy or fail otherwise. | true
| qnode.host | The host this QNode will run on. Note: localhost will be automatically substituted by the first valid private IP address at runtime. | localhost
| qnode.versions.per.tablespace | The number of succeessfully deployed versions that will be kept in the system (per tablespace). | 10
| qnode.deploy.seconds.to.check.error | The number of seconds to wait before checking each time if a DNode has failed or if timeout has ocurred in the middle of a deploy. | 60
| dnode.port | This DNode's port. | 4422
| dnode.port.autoincrement | Whether this DNode should find the next available port in case "dnode.port" is busy or fail otherwise. | true
| dnode.host | This DNode's host name. Note: localhost will be automatically substituted by the first valid private IP address at runtime. | localhost
| dnode.serving.threads	| How many threads will be allocated for serving requests in Thrift's ThreadPool Server. | 64
| dnode.data.folder	| The data folder that will be used for storing deployed SQL data stores | ./dnode-staging
| dnode.pool.cache.seconds | The amount of seconds that the DNode will cache SQL connection pools. After that time, it will close them. Remember that the DNode may receive requests for different versions in the middle of a deployment, so that's why we want to expire connection pools after some time (to not cache connection pools that will not be used anymore). | 3600
| dnode.pool.cache.n.elements | Number of SQL connection pools that will be cached. There will be one SQL connection pool for each tablespace, version and partition that this DNode serves. So this number must not be smaller than the different numbers of tablespace + version + partitions. | 128
| dnode.deploy.timeout.seconds | The amount of seconds that the DNode will wait before canceling a too-long deployment. Default is 10 hours. | 36000
| dnode.max.results.per.query | A hard limit on the number of results per each SQL query that this DNode may send back to QNodes. | 50000
| dnode.handle.test.commands | If set, this DNode will listen for test commands. This property is used to activating responsiveness to some commands that are useful for integration testing: making a DNode shutdown, etc. | false
| dnode.max.query.time | Queries that run for more than this time will be interrupted. Must be greater than 1000. | 15000
| dnode.slow.query.abs.limit | In milliseconds, queries that are slower will be logged with a WARNING. | 2500
| fetcher.s3.access.key	| If using S3 fetching, specify here your AWS credentials. Uncomment when needed. | (none)
| fetcher.s3.secret.key	| If using S3 fetching, specify here your AWS credentials. Uncomment when needed. | (none)
| fetcher.temp.dir | The local folder that will be used to download new deployments. | fetcher-tmp
| fetcher.download.buffer | The size in bytes of the in-memory buffer used to download files from S3. | 1048576
| fetcher.hadoop.fs.name | If using Hadoop fetching, the address of the NameNode for being able download data from HDFS. Uncomment when needed. | (none)
| hz.persistent.data.folder | Folder to be used to persist Hazelcast state information needed to persist current version information. If not present, no information is stored. | hz-data
| hz.port | Enable this property if you want your service to bind to an specific port. Otherwise the default Hazelcast port is used (5701), and auto-incremented if needed. | (none)
| hz.join.method | Use this property to configure Hazelcast join in one or other way. Possible values: MULTICAST, TCP, AWS | multicast
| hz.multicast.group | Uncomment and use this property if method=MULTICAST and fine-tuning is needed. | (none)
| hz.multicast.port | Uncomment and use this property if method=MULTICAST and fine-tuning is needed. | (none)
| hz.tcp.cluster | Uncomment and use this property if method=TCP. Specify a comma-separated list of host:ip cluster members. | (none)
| hz.aws.security.group | Uncomment and use this property if method=AWS and only a certain security group is to be examined. | (none)
| hz.aws.key | Don't forget your AWS credentials if you use method=AWS. | (none)
| hz.aws.secret | Don't forget your AWS credentials if you use method=AWS. | (none)
| hz.backup.count | Modifies the standard backup count. Affects the replication factor of distributed maps. | 3
| hz.disable.wait.when.joining | Hazelcast waits 5 seconds before joining a member. That is good in production because improves the posibilities of joining several members at the same time. But very bad for testing... This property allows you to disable it for testing. | false
| hz.oldest.members.leading.count | Number of the oldest members leading operations in the cluster. Sometimes only these members answer to events, in order to reduce coordination traffic. | 3
| hz.registry.max.time.to.check.registration | Max time, in minutes, to check if the member is registered. This check is used to assure eventual consistency in rare cases of network partitions where replication was not enough to ensure that no data is lost. | 5
| dnode.db.connections.per.pool | Size of the connection pool to each partition that this DNode services. | 10
|=======================

Typical distributed configurations
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

It is fairy easy to install Splout in a distributed environment. By default, Splout will use Hazelcast in Multicast mode for finding members, but it is possible to configure Splout for explicit TCP/IP or Amazon AWS auto-discovery. Following there are some examples of distributed configurations:

.Distributed configurations
|=======================
| Multicast | "hz.join.method=aws", Activated by default. | Optionally, "hz.multicast.group", "hz.multicast.port" can be used for fine-tuning the configuration.
| TCP/IP | "hz.join.method=tcp", "hz.tcp.cluster=192.168.1.3,192.168.1.4" | Only hosts specified in the comma-separated list will be considered for membership.
| AWS | "hz.join.method=aws", "hz.aws.key=KEY", "hz.aws.secret=SECRET" | Using the provided credentials, active hosts in AWS will be considered for membership. The list of hosts can be narrowed by specifying a security group in " hz.aws.security.group"
|=======================

Splout-Hadoop API
-----------------

The Splout-Hadoop API contains the libraries and command-line tools needed for deploying tablespaces to Splout.

Command line tools
~~~~~~~~~~~~~~~~~~

The command line tools have been developed to ease the most common use cases. They only work with textual (CSV or fixed-width) files at this moment.
There are two "generator" tools that are responsible for launching a Hadoop process that will balance and index the input files,
and there is one "deployer" tool that is able to deploy the result of any of the generators to an alive Splout cluster.

Simple generator
^^^^^^^^^^^^^^^^

The "Simple generator" allows us to seamlessly index and deploy a single tablespace made up by a single table, which is a very common use case. 
By invoking the tool with no parameters we obtain an explanation of all possible parameters. We will see a few examples of how to use this tool:

The following line generates the structures needed for deploying a tablespace called "customers" containing a table named "customers" whose schema is made up by an integer "customer_id" field, a "name" string and an integer "age". The file is present in input folder "my-input", will be partitioned in 12 partitions and the binary resultant files will be saved in "my-output":

	hadoop jar splout-hadoop-0.1-SNAPSHOT-hadoop.jar simple-generate -i my-input -o my-output -pby customer_id -p 12 -s "customer_id:int,name:string,age:int" -t customers -tb customers
 
.Default text format
TIP: The default text format, when not specified, is a tabulated file with no quotes, no escaping, no header and no other active advanced parsing option.

The following line generates the structures for the same tablespace, but specifying a custom CSV format which is comma-separated, escaped by character "\", uses strict quotes (""), has a header line and may contain a sequence of characters which has to be interpreted as null: "\N".

	hadoop jar splout-hadoop-0.1-SNAPSHOT-hadoop.jar simple-generate --separator , --escape \\ --quotes \"\"\" --nullstring \\N -i my-input -o my-output -pby customer_id -p 12 -s "customer_id:int,name:string,age:int" -t customers -tb customers
 
WARNING: Notice how we needed to escape backslashes when passing them through command-line parameters.

.Fixed-width textual format
TIP: Splout can also use fixed-width text files. For that, you can use the argument "--fixedwidthfields". When used, you must provide a comma-separated list of numbers. These numbers will be interpreted by pairs, as [beginning, end] inclusive position offsets. For example: "0,3,5,7" means there are two fields, the first one of 4 characters at offsets [0, 3] and the second one of 3 characters at offsets [5, 7].  

Generator
^^^^^^^^^

The "generator" is a simpler command-line which only accepts a JSON file. This JSON file will contain the specification of the tablespace or tablespaces to generate. In this case, tablespace specs can be as complex as desired, containing multiple tables if needed. You can also provide more than one JSON tablespace file to generate them together. Following we will show an example tablespace JSON file:

  {
    "name": "meteo",
    "nPartitions": 16,
    "partitionedTables": [{
      "name": "meteo",
      "schema": "station:string,date:string,metric:string,measure:int",
      "partitionFields": "station",
      "tableInputs": [{
          "inputSpecs": {
          "separatorChar": ","
        },
        "paths": [ "small.csv" ]
      }]	
    }]
  }

Following we will show the full schema of the JSON object (JSONTablespaceDefinition) that can be passed through this file:

.JSONTablespaceDefinition spec
|==============
| *Property* | *Type* | *Explanation*
| name | string | The name of the tablespace.
| nPartitions | integer | The number of partitions to generate.
| partitionedTables | array of JSONTableDefinition | The partitioned tables of this tablespace. There must be one, at least.
| replicateAllTables | array of JSONTableDefinition | The tables that are replicated to all the partitions.
|==============

This is the spec of the JSONTableDefinition object:

.JSONTableDefinition object
|==============
| *Property* | *Type* | *Explanation*
| name | string | The name of the table.
| tableInputs | array of JSONTableInputDefinition | The input locations of this table.
| schema | string | The in-line Pangool schema that defines the structure of this table.
| partitionFields | array of string | If used, the table will be partitioned by one or more columns, otherwise it will be replicated to all partitions.
| indexes | array of string | List of columns that need to be indexed after the data is added to the table. You can also specify compound indexes here, comma-separated. 
| preInsertStatements | array of string | Raw SQL commands that will be performed before inserting all the data to the table.
| finalStatements | array of string | Raw SQL commands that will be performed after inserting all the data to the table.
|==============

This is the spec of the JSONTableInputDefinition object:

.JSONTableInputDefinition object
|==============
| *Property* | *Type* | *Explanation*
| inputSpecs | TextInputSpecs | Specifies how to parse the text file.
| paths | array of string | List of paths that will be used for creating this table.
|==============

And this is the spec of the TextInputSpecs object:

.TextInputSpecs object
|==============
| *Property* | *Type* | *Explanation*
| separatorChar | character | The field separator in the file. By default, a tabulation.
| quotesChar | character | The quotes character, if any. By default, none.
| escapeChar | character | The character used for escaping, if any. By default, none.
| skipHeader | boolean | If the CSV has a header, activate this property for not failing to parse it.
| strictQuotes | boolean | If quotesChar is specified, activating this property will cause all the fields without quotes to be considered null. False by default.
| nullString | string | A sequence of characters that, if found without quotes, will be considered null. None by default.
| fixedWidthFields | array of integers | If present, the file will be parsed as a fixed-width file. When used, you must provide a comma-separated list of numbers. These numbers will be interpreted by pairs, as [beginning, end] inclusive position offsets. For example: "0,3,5,7" means there are two fields, the first one of 4 characters at offsets [0, 3] and the second one of 3 characters at offsets [5, 7].
|==============

Deployer
^^^^^^^^

The "deployer" tool can be used for deploying any tablespace or set of tablespaces that has been generated by any of the generators. More than one tablespace may be deployed at the same time, and Splout will increment the version for all of them in an "all-or-nothing" fashion.
For the common case of deploying only one tablespace, you can use straight command-line parameters:

 hadoop jar splout-hadoop-0.1-SNAPSHOT-hadoop.jar deploy -r 2 -root my-generated-tablespace -tn mytablespace -q http://localhost:4412

The above line will deploy binary files generated in "my-generated-tablespace" folder using replication 2. The deployed tablespace will be named "mytablespace" and it will be deployed to the alive Splout cluster using the local QNode address at port 4412. 
The corresponding file tree for this example would have been the following:

 hdfs://.../my-generated-tablespace/
 hdfs://.../my-generated-tablespace/partition-map
 hdfs://.../my-generated-tablespace/sampled-input
 hdfs://.../my-generated-tablespace/store

.Replication
TIP: For failover, it is convenient to replicate your tablespace when deploying it. If ommitted, only one copy of each binary file will be distributed to the cluster, meaning that if one machine fails there will be a portion of your data that will not be available for serving. A replication factor of 2 will mean that there will be 2 copies of each file, so one machine can fail and all the data will still be served. When deploying to a cluster with less machines than the replication factor specified, it will be automatically downgraded to the minimum viable one.

For deploying more than one tablespace with the same replication factor, you can also use command-line parameters:

 hadoop jar splout-hadoop-0.1-SNAPSHOT-hadoop.jar deploy -r 2 -root my-root-folder -tn mytablespace1,mytablespace2,mytablespace3 -q http://localhost:4412

In this case we will deploy 3 tablespaces at the same time: mytablespace1, mytablespace2 and mytablespace3. The "root" parameter is a parent folder that contains the specified subfolders, and the tablespaces will be named after the folder name. So in this case the file tree structure is the following:

 hdfs://.../my-root-folder/mytablespace1/partition-map
 hdfs://.../my-root-folder/mytablespace1/sampled-input
 hdfs://.../my-root-folder/mytablespace1/store/...
 hdfs://.../my-root-folder/mytablespace2/partition-map
 hdfs://.../my-root-folder/mytablespace2/sampled-input
 hdfs://.../my-root-folder/mytablespace2/store/...
 hdfs://.../my-root-folder/mytablespace3/partition-map
 hdfs://.../my-root-folder/mytablespace3/sampled-input
 hdfs://.../my-root-folder/mytablespace3/store/...
 
Last but not least, if we are to deploy a more complex combination of tablespaces, we can also use a JSON configuration file for that. This file will contain an array of "TablespaceDepSpec" objects whose spec is the following:

.TablespaceDepSpec object
|================
| *Property* | *Type* | *Explanation*
| sourcePath | string | The root folder which contains one or more tablespaces and where this tablespace can be located. 
| tablespace | string | The subfolder in the root folder that contains the tablespace. It will be used for its name.
| replication | integer | The replication factor to be used for this tablespace.
|================

In this case, we can just pass the configuration file like shown below, taking into account that the file must be present in the local file system:

 hadoop jar splout-hadoop-0.1-SNAPSHOT-hadoop.jar -c my-config.json -q http://localhost:4412

Hadoop Java API
~~~~~~~~~~~~~~~

All the command-line tools use the underlying Java API that we have implemented for Splout. You can also use this Java API directly in your Java project and you can have access to more advanced features such specifying custom partitioning functions, record processing functions and such.

Basic API
^^^^^^^^^

The basic API consists of the following classes:

* TableBuilder: a builder used to obtain a Table instance. Table instances can be used in TablespaceBuilder for constructing a tablespace specification.
* TablespaceBuilder: a builder used to obtain a TablespaceSpec instance.
* TablespaceGenerator: It can be used to generate the binary files according to a TablespaceSpec instance.
* StoreDeployerTool: It can be used to deploy the files generated by the TablespaceGenerator to an alive Splout cluster. It will accept TablespaceDepSpec instances, which have been documented in the previous sections. 

The javadoc of each of these classes should guide you well into using them in your custom Java project. 

Custom partitioning
+++++++++++++++++++

Aside of column-based partitioning, an arbitrary partitioning function can be provided in the form of a Javascript function. This function ca be passed to TableBuilder's partitionByJavaScript() method. 

RecordProcessor
+++++++++++++++

If you want to have more control on the generation process, you can implement your own RecordProcessor which will receive Tuples as they have been parsed by the input files and should emit Tuples as you want them to be indexed in your SQL tables.
For example, you may choose to narrow your input Tuple and emit a subset of it, modify some field by decoding its content, and so on. The RecordProcessor may also act as a filter. If "null" is returned, the input Tuple would have been filtered out from the generation process.

REST API
--------

Troubleshooting
---------------
